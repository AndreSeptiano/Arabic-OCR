{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4aa9c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU                 Memory [%]          Free                Utilization [%]     \n",
      "0                    62.94               15025 MiB           0                  \n",
      "1                    97.55               992 MiB             0                  \n",
      "2                    85.52               5869 MiB            17                 \n",
      "3                    57.75               17126 MiB           2                  \n",
      "4                    95.74               1728 MiB            85                 \n",
      "5                    69.39               12407 MiB           0                  \n",
      "6                    78.50               8717 MiB            84                 \n",
      "7                    84.64               6228 MiB            0                  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi --query-gpu=index,memory.used,memory.total,memory.free,utilization.gpu --format=csv,noheader | awk -F, '\n",
    "BEGIN {\n",
    "    printf \"GPU,Memory [%%],Free,Utilization [%%]\\n\"\n",
    "}\n",
    "{\n",
    "    printf \"%s,%6.2f,%-12s,%-12s\\n\", $1, ($2/$3)*100, $4, $5\n",
    "}' | sed 's|\\s%||g' | awk -F, '\n",
    "BEGIN {\n",
    "    OFS=\"\\t\";   # Output field separator, adjust as needed\n",
    "}\n",
    "{\n",
    "    for (i=1; i<=NF; i++) {\n",
    "        printf \"%-20s\", $i;   # Adjust the width (20 in this example) as needed\n",
    "    }\n",
    "    print \"\";   # Print a new line after each row\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih GPU yang akan digunakan (contohnya: GPU #7)\n",
    "import os\n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc755010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 edit.py {output_dir} {truth_dir}   # Edit Distance between OCR Result and Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52e3688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries, constants, singletons, and functions\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import base64\n",
    "import pickle\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from character_segmentation import segment\n",
    "from segmentation import extract_words\n",
    "from train import prepare_char, featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8597ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_dir = '/workspace/Dataset/pegon-annotated-dataset'\n",
    "tokens_to_unknown = ['[CALLIGRAPHY]',\n",
    "                     '[NASTALIQ]',\n",
    "                     '[UNKNOWN]',\n",
    "                     '[VERT]',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "838a48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_image_from_json(json_path):\n",
    "    with open(json_path, encoding=\"utf8\") as jsonfile:\n",
    "        json_obj = json.load(jsonfile)\n",
    "    \n",
    "    filename    = json_obj['imagePath']\n",
    "    encoded_img = json_obj['imageData']\n",
    "    image_arr   = np.frombuffer(base64.b64decode(encoded_img), np.uint8)\n",
    "    image       = cv.imdecode(image_arr, cv.IMREAD_COLOR)\n",
    "    return filename, image\n",
    "\n",
    "def clear_running_time(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    open(f'{path}/running_time.txt', 'w').close()\n",
    "    return\n",
    "\n",
    "def write_running_time(running_time_list, path):\n",
    "    running_time_list.sort()\n",
    "    with open(f'{path}/running_time.txt', 'w') as r:\n",
    "        for t in running_time_list:\n",
    "            r.writelines(f'image#{t[0]}: {t[1]}\\n')       # if no need for printing 'image#id'.\n",
    "    return\n",
    "\n",
    "def extract_ground_truth(json_path):\n",
    "    with open(json_path, encoding=\"utf8\") as jsonfile:\n",
    "        json_obj = json.load(jsonfile)\n",
    "    filename   = json_obj['imagePath']\n",
    "    \n",
    "    clean_word = ' '.join([hashmap['label'] for hashmap in json_obj['shapes']])\n",
    "    return filename, clean_word\n",
    "\n",
    "def run_pool(obj, model=model):\n",
    "    word, line = obj\n",
    "    char_imgs = segment(line, word)\n",
    "    txt_word = []\n",
    "    for char_img in char_imgs:\n",
    "        try:\n",
    "            ready_char = prepare_char(char_img)\n",
    "        except:\n",
    "            continue\n",
    "        feature_vector = featurizer(ready_char)\n",
    "        predicted_char = model.predict([feature_vector])[0]\n",
    "        txt_word.append(predicted_char)\n",
    "    return ''.join(txt_word)\n",
    "\n",
    "def run_ocr(json_path, current_folder):\n",
    "    image_name, image = get_image_from_json(json_path)\n",
    "    \n",
    "    # Start\n",
    "    before = time.time()\n",
    "    words = extract_words(image)\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    predicted_words = pool.map(run_pool, words)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Stop Timer\n",
    "    after = time.time()\n",
    "    predicted_text = ' '.join(predicted_words)\n",
    "    exc_time = after-before\n",
    "    \n",
    "    with open(f'{result_dir}/{current_folder}/text/{image_name}.txt','w',encoding='utf8') as fo:\n",
    "        fo.writelines(predicted_text)\n",
    "        \n",
    "    return image_name, exc_time\n",
    "\n",
    "def run_folder(folder):\n",
    "    clear_running_time(f'{result_dir}/{folder}')\n",
    "    destination = f'{result_dir}/{folder}/text'\n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "    json_paths = glob(f'{dataset_dir}/{folder}/*.json')\n",
    "    \n",
    "    running_time = []\n",
    "    before = time.time()\n",
    "    for json_path in tqdm(json_paths,total=len(json_paths)):\n",
    "        result = run_ocr(json_path,folder)\n",
    "        running_time.append(result)\n",
    "    write_running_time(running_time,f'{result_dir}/{folder}')\n",
    "    \n",
    "    after = time.time()\n",
    "    print()\n",
    "    print(f'total time to finish {len(running_time)} images: {after - before}')\n",
    "    print(f'Successfully processing {len(running_time)} out of {len(json_paths)} images')\n",
    "    print()\n",
    "    \n",
    "def main():\n",
    "    folders = ['Majmuah Syariah','Mujarobat Doa','Mujarobat Kubro']\n",
    "    for folder in folders:\n",
    "        run_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "466e4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Error Rate for evaluation\n",
    "from jiwer import cer, wer\n",
    "\n",
    "def eval_cer(filename,folder,result_dir):\n",
    "    filename = filename.split('/')[-1].split('.')[0]\n",
    "    pred = open(f'{result_dir}/{folder}/text/{filename}.bmp.txt','r')\n",
    "    _, true = extract_ground_truth(f'{dataset_dir}/{folder}/{filename}.json')\n",
    "    return f'{folder}/{filename}', cer(true, pred.read())\n",
    "\n",
    "def eval_wers(filename,folder,result_dir):\n",
    "    filename = filename.split('/')[-1].split('.')[0]\n",
    "    pred = open(f'{result_dir}/{folder}/text/{filename}.bmp.txt','r')\n",
    "    _, true = extract_ground_truth(f'{dataset_dir}/{folder}/{filename}.json')\n",
    "    return f'{folder}/{filename}', wer(true, pred.read())\n",
    "\n",
    "def get_cer_avg(result_dir):\n",
    "    cers = []\n",
    "    folders = ['Majmuah Syariah','Mujarobat Doa','Mujarobat Kubro']\n",
    "    for folder in folders:\n",
    "        json_files = glob(f'{dataset_dir}/{folder}/*.json')\n",
    "        cers.extend([eval_cer(filename,folder,result_dir) for filename in json_files])\n",
    "\n",
    "    ev = [ev for _,ev in cers]\n",
    "    return sum(ev) / len(ev)\n",
    "\n",
    "def get_wer_avg(result_dir):\n",
    "    wers = []\n",
    "    folders = ['Majmuah Syariah','Mujarobat Doa','Mujarobat Kubro']\n",
    "    for folder in folders:\n",
    "        json_files = glob(f'{dataset_dir}/{folder}/*.json')\n",
    "        wers.extend([eval_wers(filename,folder,result_dir) for filename in json_files])\n",
    "\n",
    "    ev = [ev for _,ev in wers]\n",
    "    return sum(ev) / len(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cadd14d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: 1L-NN\n",
    "result_dir  = '/workspace/Arabic-OCR/src/pegon-result-page-1lnn'\n",
    "model_name = '1L_NN.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d461df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR with Model above\n",
    "model = pickle.load(open(f'models/{model_name}','rb'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "13e1f3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CER:0.9999789578363117\n",
      "Average WER:1.0\n"
     ]
    }
   ],
   "source": [
    "# Eval result\n",
    "print(f'Average CER:{get_cer_avg(result_dir)}\\nAverage WER:{get_wer_avg(result_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8094ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Model: 2L-NN\n",
    "result_dir  = '/workspace/Arabic-OCR/src/pegon-result-page-2lnn'\n",
    "model_name = '2L_NN.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4d28d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8/15 [03:19<02:56, 25.19s/it]"
     ]
    }
   ],
   "source": [
    "# Run OCR with Model above\n",
    "model = pickle.load(open(f'models/{model_name}','rb'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5665b140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval result\n",
    "print(f'Average CER:{get_cer_avg(result_dir)}\\nAverage WER:{get_wer_avg(result_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3014440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Model: GaussNB\n",
    "result_dir  = '/workspace/Arabic-OCR/src/pegon-result-page-gaussnb'\n",
    "model_name = 'Gaussian_Naive_Bayes.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e45486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR with Model above\n",
    "model = pickle.load(open(f'models/{model_name}','rb'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a180374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval result\n",
    "print(f'Average CER:{get_cer_avg(result_dir)}\\nAverage WER:{get_wer_avg(result_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b3dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Model: LinearSVM\n",
    "result_dir  = '/workspace/Arabic-OCR/src/pegon-result-page-linsvm'\n",
    "model_name = 'LinearSVM.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa28945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR with Model above\n",
    "model = pickle.load(open(f'models/{model_name}','rb'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a7c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval result\n",
    "print(f'Average CER:{get_cer_avg(result_dir)}\\nAverage WER:{get_wer_avg(result_dir)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
