{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4aa9c7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU                 Memory [%]          Free                Utilization [%]     \n",
      "0                    92.82               2909 MiB            0                  \n",
      "1                    97.87               863 MiB             0                  \n",
      "2                    95.76               1720 MiB            3                  \n",
      "3                    35.19               26271 MiB           98                 \n",
      "4                    95.35               1884 MiB            0                  \n",
      "5                    81.75               7399 MiB            0                  \n",
      "6                    98.49               614 MiB             0                  \n",
      "7                    53.50               18851 MiB           8                  \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "nvidia-smi --query-gpu=index,memory.used,memory.total,memory.free,utilization.gpu --format=csv,noheader | awk -F, '\n",
    "BEGIN {\n",
    "    printf \"GPU,Memory [%%],Free,Utilization [%%]\\n\"\n",
    "}\n",
    "{\n",
    "    printf \"%s,%6.2f,%-12s,%-12s\\n\", $1, ($2/$3)*100, $4, $5\n",
    "}' | sed 's|\\s%||g' | awk -F, '\n",
    "BEGIN {\n",
    "    OFS=\"\\t\";   # Output field separator, adjust as needed\n",
    "}\n",
    "{\n",
    "    for (i=1; i<=NF; i++) {\n",
    "        printf \"%-20s\", $i;   # Adjust the width (20 in this example) as needed\n",
    "    }\n",
    "    print \"\";   # Print a new line after each row\n",
    "}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1486792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memilih GPU yang akan digunakan (contohnya: GPU #7)\n",
    "import os\n",
    "import torch\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc755010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 edit.py {output_dir} {truth_dir}   # Edit Distance between OCR Result and Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52e3688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries, constants, singletons, and functions\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import base64\n",
    "import pickle\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from character_segmentation import segment\n",
    "from segmentation import extract_words\n",
    "from train import prepare_char, featurizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8597ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "dataset_dir = '/workspace/Dataset/pegon-annotated-dataset'\n",
    "tokens_to_unknown = ['[CALLIGRAPHY]',\n",
    "                     '[NASTALIQ]',\n",
    "                     '[UNKNOWN]',\n",
    "                     '[VERT]',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "838a48fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "def get_image_from_json(json_path):\n",
    "    with open(json_path, encoding=\"utf8\") as jsonfile:\n",
    "        json_obj = json.load(jsonfile)\n",
    "    \n",
    "    filename    = json_obj['imagePath']\n",
    "    encoded_img = json_obj['imageData']\n",
    "    image_arr   = np.frombuffer(base64.b64decode(encoded_img), np.uint8)\n",
    "    image       = cv.imdecode(image_arr, cv.IMREAD_COLOR)\n",
    "    return filename, image\n",
    "\n",
    "def clear_running_time(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    open(f'{path}/running_time.txt', 'w').close()\n",
    "    return\n",
    "\n",
    "def write_running_time(running_time_list, path):\n",
    "    running_time_list.sort()\n",
    "    with open(f'{path}/running_time.txt', 'w') as r:\n",
    "        for t in running_time_list:\n",
    "            r.writelines(f'image#{t[0]}: {t[1]}\\n')       # if no need for printing 'image#id'.\n",
    "    return\n",
    "\n",
    "def extract_ground_truth(json_path):\n",
    "    with open(json_path, encoding=\"utf8\") as jsonfile:\n",
    "        json_obj = json.load(jsonfile)\n",
    "    filename   = json_obj['imagePath']\n",
    "    \n",
    "    clean_word = ' '.join([hashmap['label'] for hashmap in json_obj['shapes']])\n",
    "    return filename, clean_word\n",
    "\n",
    "def run_pool(obj, model=model):\n",
    "    word, line = obj\n",
    "    char_imgs = segment(line, word)\n",
    "    txt_word = []\n",
    "    for char_img in char_imgs:\n",
    "        try:\n",
    "            ready_char = prepare_char(char_img)\n",
    "        except:\n",
    "            continue\n",
    "        feature_vector = featurizer(ready_char)\n",
    "        predicted_char = model.predict([feature_vector])[0]\n",
    "        txt_word.append(predicted_char)\n",
    "    return ''.join(txt_word)\n",
    "\n",
    "def run_ocr(json_path, current_folder):\n",
    "    image_name, image = get_image_from_json(json_path)\n",
    "    \n",
    "    # Start\n",
    "    before = time.time()\n",
    "    words = extract_words(image)\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    predicted_words = pool.map(run_pool, words)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    # Stop Timer\n",
    "    after = time.time()\n",
    "    predicted_text = ' '.join(predicted_words)\n",
    "    exc_time = after-before\n",
    "    \n",
    "    with open(f'{result_dir}/{current_folder}/text/{image_name}.txt','w',encoding='utf8') as fo:\n",
    "        fo.writelines(predicted_text)\n",
    "        \n",
    "    return image_name, exc_time\n",
    "\n",
    "def run_folder(folder):\n",
    "    clear_running_time(f'{result_dir}/{folder}')\n",
    "    destination = f'{result_dir}/{folder}/text'\n",
    "    \n",
    "    if not os.path.exists(destination):\n",
    "        os.makedirs(destination)\n",
    "    json_paths = glob(f'{dataset_dir}/{folder}/*.json')\n",
    "    \n",
    "    running_time = []\n",
    "    before = time.time()\n",
    "    for json_path in tqdm(json_paths,total=len(json_paths)):\n",
    "        result = run_ocr(json_path,folder)\n",
    "        running_time.append(result)\n",
    "    write_running_time(running_time,f'{result_dir}/{folder}')\n",
    "    \n",
    "    after = time.time()\n",
    "    print()\n",
    "    print(f'total time to finish {len(running_time)} images: {after - before}')\n",
    "    print(f'Successfully processing {len(running_time)} out of {len(json_paths)} images')\n",
    "    print()\n",
    "    \n",
    "def main():\n",
    "    folders = ['Majmuah Syariah','Mujarobat Doa','Mujarobat Kubro']\n",
    "    for folder in folders:\n",
    "        run_folder(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e7c75b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Error Rate for evaluation\n",
    "from jiwer import cer, wer\n",
    "\n",
    "def eval_cer(filename,folder,result_dir):\n",
    "    filename = filename.split('/')[-1].split('.')[0]\n",
    "    pred = open(f'{result_dir}/{folder}/text/{filename}.bmp.txt','r')\n",
    "    _, true = extract_ground_truth(f'{dataset_dir}/{folder}/{filename}.json')\n",
    "    return f'{folder}/{filename}', cer(true, pred.read())\n",
    "\n",
    "def eval_wers(filename,folder,result_dir):\n",
    "    filename = filename.split('/')[-1].split('.')[0]\n",
    "    pred = open(f'{result_dir}/{folder}/text/{filename}.bmp.txt','r')\n",
    "    _, true = extract_ground_truth(f'{dataset_dir}/{folder}/{filename}.json')\n",
    "    return f'{folder}/{filename}', wer(true, pred.read())\n",
    "\n",
    "def get_cer_avg(result_dir):\n",
    "    cers = []\n",
    "    folders = ['Majmuah Syariah','Mujarobat Doa','Mujarobat Kubro']\n",
    "    for folder in folders:\n",
    "        json_files = glob(f'{dataset_dir}/{folder}/*.json')\n",
    "        cers.extend([eval_cer(filename,folder,result_dir) for filename in json_files])\n",
    "\n",
    "    ev = [ev for _,ev in cers]\n",
    "    return sum(ev) / len(ev)\n",
    "\n",
    "def get_wer_avg(result_dir):\n",
    "    wers = []\n",
    "    folders = ['Majmuah Syariah','Mujarobat Doa','Mujarobat Kubro']\n",
    "    for folder in folders:\n",
    "        json_files = glob(f'{dataset_dir}/{folder}/*.json')\n",
    "        wers.extend([eval_wers(filename,folder,result_dir) for filename in json_files])\n",
    "\n",
    "    ev = [ev for _,ev in wers]\n",
    "    return sum(ev) / len(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "092392f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: 1L-NN\n",
    "result_dir  = '/workspace/Arabic-OCR/src/pegon-result-page-1lnn'\n",
    "model_name = '1L_NN.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7553eaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run OCR with Model above\n",
    "model = pickle.load(open(f'models/{model_name}','rb'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "267d98fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CER:0.9999789578363117\n",
      "Average WER:1.0\n"
     ]
    }
   ],
   "source": [
    "# Eval result\n",
    "print(f'Average CER:{get_cer_avg(result_dir)}\\nAverage WER:{get_wer_avg(result_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "012aed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Model: 2L-NN\n",
    "result_dir  = '/workspace/Arabic-OCR/src/pegon-result-page-2lnn'\n",
    "model_name = '2L_NN.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a4d28d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [06:13<00:00, 24.87s/it]\n",
      "  0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time to finish 15 images: 373.0619041919708\n",
      "Successfully processing 15 out of 15 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [20:21<00:00, 24.93s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time to finish 49 images: 1221.8094124794006\n",
      "Successfully processing 49 out of 49 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [20:50<00:00, 25.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time to finish 50 images: 1250.1710419654846\n",
      "Successfully processing 50 out of 50 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run OCR with Model above\n",
    "model = pickle.load(open(f'models/{model_name}','rb'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d4fb300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CER:0.9999763291489205\n",
      "Average WER:1.0\n"
     ]
    }
   ],
   "source": [
    "# Eval result\n",
    "print(f'Average CER:{get_cer_avg(result_dir)}\\nAverage WER:{get_wer_avg(result_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32e63a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Model: GaussNB\n",
    "result_dir  = '/workspace/Arabic-OCR/src/pegon-result-page-gaussnb'\n",
    "model_name = 'Gaussian_Naive_Bayes.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c595a153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [06:13<00:00, 24.91s/it]\n",
      "  0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time to finish 15 images: 373.6137704849243\n",
      "Successfully processing 15 out of 15 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [20:17<00:00, 24.84s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time to finish 49 images: 1217.3779122829437\n",
      "Successfully processing 49 out of 49 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [20:44<00:00, 24.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time to finish 50 images: 1244.905256986618\n",
      "Successfully processing 50 out of 50 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run OCR with Model above\n",
    "model = pickle.load(open(f'models/{model_name}','rb'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22cd75e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CER:0.9999763291489205\n",
      "Average WER:1.0\n"
     ]
    }
   ],
   "source": [
    "# Eval result\n",
    "print(f'Average CER:{get_cer_avg(result_dir)}\\nAverage WER:{get_wer_avg(result_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "205d120d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next Model: LinearSVM\n",
    "result_dir  = '/workspace/Arabic-OCR/src/pegon-result-page-linsvm'\n",
    "model_name = 'LinearSVM.sav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ccadf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [06:14<00:00, 24.95s/it]\n",
      "  0%|          | 0/49 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time to finish 15 images: 374.3252499103546\n",
      "Successfully processing 15 out of 15 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [20:12<00:00, 24.75s/it]\n",
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time to finish 49 images: 1212.7852368354797\n",
      "Successfully processing 49 out of 49 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [20:39<00:00, 24.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "total time to finish 50 images: 1239.915227651596\n",
      "Successfully processing 50 out of 50 images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run OCR with Model above\n",
    "model = pickle.load(open(f'models/{model_name}','rb'))\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d5c339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CER:0.9999763291489205\n",
      "Average WER:1.0\n"
     ]
    }
   ],
   "source": [
    "# Eval result\n",
    "print(f'Average CER:{get_cer_avg(result_dir)}\\nAverage WER:{get_wer_avg(result_dir)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9df5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Line Segmentation Method\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
